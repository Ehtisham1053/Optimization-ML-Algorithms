{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+nxTwvsEPOTOKZgW8eOoX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ehtisham1053/Optimization-ML-Algorithms/blob/main/Batch_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will learn about the batch gradient descent and then apply this to find the values of m and b in multiple linear regression"
      ],
      "metadata": {
        "id": "Uq010FPL3GmC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DdPxdKHc3B86"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('audi.csv')\n",
        "x = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "c = ColumnTransformer([('encoder', OneHotEncoder(handle_unknown='ignore' , sparse_output=False , drop='first'), ['transmission', 'fuelType', 'model']),\n",
        "                       ('scaler', StandardScaler(), ['mileage', 'tax', 'mpg', 'engineSize', 'year'])\n",
        "\n",
        "                       ], remainder='passthrough')\n",
        "\n",
        "x_train = c.fit_transform(x_train)\n",
        "x_test = c.transform(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQOxr2P4ah0",
        "outputId": "060c00a5-7028-4f1d-ebf1-7325f0e5cc4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Gradient descent Class"
      ],
      "metadata": {
        "id": "OCDVBcYk5UeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BatchGradientDescent:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.m = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.m = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            y_pred = np.dot(X, self.m) + self.b\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Compute gradients\n",
        "            dm = (1/n_samples) * np.dot(X.T, error)\n",
        "            db = (1/n_samples) * np.sum(error)\n",
        "\n",
        "            # Update parameters\n",
        "            self.m -= self.learning_rate * dm\n",
        "            self.b -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.m) + self.b\n",
        "\n",
        "    def mse(self, X, y):\n",
        "        return np.mean((self.predict(X) - y) ** 2)\n",
        "\n",
        "    def mae(self, X, y):\n",
        "        return np.mean(np.abs(self.predict(X) - y))\n",
        "\n",
        "    def rmse(self, X, y):\n",
        "        return np.sqrt(self.mse(X, y))\n",
        "\n",
        "    def r2_score(self, X, y):\n",
        "        y_mean = np.mean(y)\n",
        "        ss_total = np.sum((y - y_mean) ** 2)\n",
        "        ss_residual = np.sum((y - self.predict(X)) ** 2)\n",
        "        return 1 - (ss_residual / ss_total)\n",
        "\n",
        "    def adjusted_r2(self, X, y):\n",
        "        n, k = X.shape\n",
        "        r2 = self.r2_score(X, y)\n",
        "        return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n"
      ],
      "metadata": {
        "id": "v9iJPRiq5XvB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BatchGradientDescent(learning_rate=0.1, epochs=1000)\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print(\"MSE:\", model.mse(x_test, y_test))\n",
        "print(\"MAE:\", model.mae(x_test, y_test))\n",
        "print(\"RMSE:\", model.rmse(x_test, y_test))\n",
        "print(\"R² Score:\", model.r2_score(x_test, y_test))\n",
        "print(\"Adjusted R² Score:\", model.adjusted_r2(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNKPQlPb5kKJ",
        "outputId": "0e87699d-a0cd-44e9-b3fe-d49e0e6e654a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 24438367.74617116\n",
            "MAE: 3033.4305302614703\n",
            "RMSE: 4943.517750162445\n",
            "R² Score: 0.8382951845367729\n",
            "Adjusted R² Score: 0.8357541088652078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Gradient Descent (BGD)\n",
        "Batch Gradient Descent computes the gradients using the entire dataset before updating the model parameters. This means each update is based on the sum of all the errors across all training samples. It ensures a smooth and stable convergence since it moves in the direction of the global minimum without much variance.\n",
        "\n",
        "##When to Use:\n",
        "BGD is ideal when the dataset is small to moderately sized, as it requires loading the entire dataset into memory at once. It works well when computational resources are sufficient and when the loss function has a smooth, convex shape.\n",
        "\n",
        "##Advantages:\n",
        "* Provides a stable convergence path and is less noisy.\n",
        "* Guarantees reaching the global minimum for convex functions.\n",
        "* Efficient when working with small datasets.\n",
        "\n",
        "##Disadvantages:\n",
        "* Computationally expensive for large datasets as it requires the entire dataset for each update.\n",
        "* Can be slow to converge, especially for high-dimensional data.\n",
        "* Not suitable for streaming data, as it needs all data at once."
      ],
      "metadata": {
        "id": "VkzLmlR39w9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "bgd_model = LinearRegression()\n",
        "bgd_model.fit(x_train, y_train)\n",
        "y_pred_bgd = bgd_model.predict(x_test)\n",
        "mse_bgd = mean_squared_error(y_test, y_pred_bgd)\n",
        "r2_bgd = r2_score(y_test, y_pred_bgd)\n",
        "\n",
        "print(\"Batch Gradient Descent Results:\")\n",
        "print(f\"Mean Squared Error: {mse_bgd}\")\n",
        "print(f\"R² Score: {r2_bgd}\")\n"
      ],
      "metadata": {
        "id": "OQUrByL15vCw",
        "outputId": "65e8eb6c-fc58-448e-aab5-a07675f752d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Gradient Descent Results:\n",
            "Mean Squared Error: 15861863.853997568\n",
            "R² Score: 0.8950445547732881\n"
          ]
        }
      ]
    }
  ]
}